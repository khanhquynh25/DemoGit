{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJPtxYSzzXh8ciMoKtOVdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhquynh25/DemoGit/blob/master/m%C3%B3n_%C4%83n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hb62E6uiuxN3",
        "outputId": "543650a3-c42f-40b5-b646-fbb769880b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 21 images belonging to 5 classes.\n",
            "Found 6 images belonging to 5 classes.\n",
            "📌 Các lớp: ['com', 'pho', 'rau', 'thit', 'trung']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.1905 - loss: 2.2343 - val_accuracy: 0.3333 - val_loss: 6.7578\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1429 - loss: 9.5995 - val_accuracy: 0.1667 - val_loss: 18.0616\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3333 - loss: 18.3504 - val_accuracy: 0.1667 - val_loss: 16.7334\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3810 - loss: 15.8688 - val_accuracy: 0.1667 - val_loss: 11.0511\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step - accuracy: 0.1429 - loss: 17.0471 - val_accuracy: 0.1667 - val_loss: 8.0219\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804ms/step - accuracy: 0.1429 - loss: 14.1513 - val_accuracy: 0.5000 - val_loss: 6.3520\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step - accuracy: 0.4762 - loss: 14.6301 - val_accuracy: 0.3333 - val_loss: 9.1326\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step - accuracy: 0.4286 - loss: 10.2286 - val_accuracy: 0.5000 - val_loss: 6.1235\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5238 - loss: 10.8508 - val_accuracy: 0.1667 - val_loss: 8.6319\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.4286 - loss: 11.8087 - val_accuracy: 0.1667 - val_loss: 7.7382\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5238 - loss: 9.5822 - val_accuracy: 0.1667 - val_loss: 5.3641\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972ms/step - accuracy: 0.2857 - loss: 9.8079 - val_accuracy: 0.3333 - val_loss: 3.3695\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.4762 - loss: 12.4992 - val_accuracy: 0.5000 - val_loss: 2.6997\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step - accuracy: 0.6190 - loss: 5.5087 - val_accuracy: 0.5000 - val_loss: 2.7986\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5238 - loss: 8.4172 - val_accuracy: 0.5000 - val_loss: 2.4273\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833ms/step - accuracy: 0.5714 - loss: 8.2935 - val_accuracy: 0.5000 - val_loss: 2.9215\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step - accuracy: 0.3810 - loss: 9.5439 - val_accuracy: 0.5000 - val_loss: 3.7187\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6190 - loss: 7.1062 - val_accuracy: 0.5000 - val_loss: 4.9004\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step - accuracy: 0.5714 - loss: 4.5781 - val_accuracy: 0.1667 - val_loss: 6.5809\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825ms/step - accuracy: 0.3810 - loss: 7.0107 - val_accuracy: 0.1667 - val_loss: 6.7871\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step - accuracy: 0.7143 - loss: 4.7309 - val_accuracy: 0.1667 - val_loss: 7.7413\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842ms/step - accuracy: 0.5714 - loss: 6.8103 - val_accuracy: 0.1667 - val_loss: 7.1996\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5238 - loss: 6.0761 - val_accuracy: 0.1667 - val_loss: 6.8312\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8571 - loss: 1.0188 - val_accuracy: 0.1667 - val_loss: 6.2809\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7619 - loss: 1.8747 - val_accuracy: 0.1667 - val_loss: 4.6632\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5238 - loss: 3.2939 - val_accuracy: 0.1667 - val_loss: 3.0540\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967ms/step - accuracy: 0.6190 - loss: 4.2351 - val_accuracy: 0.3333 - val_loss: 1.9754\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step - accuracy: 0.7619 - loss: 2.7780 - val_accuracy: 0.5000 - val_loss: 1.6316\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step - accuracy: 0.7619 - loss: 1.6527 - val_accuracy: 0.5000 - val_loss: 1.5794\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766ms/step - accuracy: 0.8571 - loss: 1.0231 - val_accuracy: 0.5000 - val_loss: 1.6444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Đã train xong và lưu mô hình ANN tại: /content/drive/MyDrive/monan/food_ann.h5\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f498a35e905ad452ba.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f498a35e905ad452ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# ================== 1. KẾT NỐI GOOGLE DRIVE ==================\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Thư mục chứa dataset trong Drive\n",
        "# Cấu trúc thư mục phải như sau:\n",
        "# /content/drive/MyDrive/monan/\n",
        "#    ├── pho/\n",
        "#    ├── com/\n",
        "#    ├── rau/\n",
        "#    ├── thit/\n",
        "#    └── trung/\n",
        "base_dir = \"/content/drive/MyDrive/monan\"\n",
        "\n",
        "# ================== 2. TIỀN XỬ LÝ DỮ LIỆU ==================\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.3  # 30% dữ liệu cho validation\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Lấy nhãn lớp\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "print(\"📌 Các lớp:\", labels)\n",
        "\n",
        "# ================== 3. XÂY DỰNG MÔ HÌNH ANN ==================\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(img_size[0], img_size[1], 3)),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(256, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(labels), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# ================== 4. COMPILE ==================\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# ================== 5. TRAIN ==================\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30\n",
        ")\n",
        "\n",
        "# ================== 6. LƯU MÔ HÌNH VÀO DRIVE ==================\n",
        "save_path = \"/content/drive/MyDrive/monan/food_ann.h5\"\n",
        "model.save(save_path)\n",
        "print(f\"✅ Đã train xong và lưu mô hình ANN tại: {save_path}\")\n",
        "\n",
        "# ================== 7. GIAO DIỆN GRADIO ==================\n",
        "\n",
        "def predict_and_show(img):\n",
        "    img_resized = img.resize(img_size)   # resize về (128,128)\n",
        "    img_array = img_to_array(img_resized) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    preds = model.predict(img_array)\n",
        "    class_idx = np.argmax(preds)\n",
        "    confidence = np.max(preds)\n",
        "    label = labels[class_idx]\n",
        "\n",
        "    return f\"🍽️ Đây là món: {label}\\n(Độ tin cậy: {confidence:.2f})\"\n",
        "\n",
        "# ================== CSS giao diện ==================\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {background-color: #fff5e6; font-family: Arial;}\n",
        "h1, h2, h3 {color: #cc6600; font-weight: bold;}\n",
        "label {color: #994d00; font-weight: bold;}\n",
        "button {background-color: #ff6600 !important; color: white !important;\n",
        "        font-weight: bold; border-radius: 10px !important; padding: 10px 20px !important;}\n",
        ".output-textbox textarea {font-size: 22px !important; font-weight: bold !important; color: #cc3300;}\n",
        "\"\"\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_and_show,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"📷 Upload ảnh món ăn\"),\n",
        "    outputs=gr.Textbox(label=\"📌 Kết quả dự đoán\"),\n",
        "    title=\"🍽️ Nhận diện món ăn Việt\",\n",
        "    description=\"Upload ảnh phở, cơm, rau, thịt hoặc trứng để nhận diện.\",\n",
        "    css=custom_css\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    }
  ]
}